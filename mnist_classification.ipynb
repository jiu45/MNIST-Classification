{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification Project\n",
    "\n",
    "## 1. Introduction & Data Loading\n",
    "\n",
    "The goal of this project is to explore supervised machine learning by building and evaluating Decision Trees and Neural Networks on the MNIST dataset. The MNIST dataset consists of 70,000 grayscale images of handwritten digits (0-9), each 28x28 pixels.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load and explore the data.\n",
    "2. Preprocess the data (normalization, flattening, splitting).\n",
    "3. Train and evaluate a Decision Tree Classifier.\n",
    "4. Perform hyperparameter tuning for the Decision Tree.\n",
    "5. Train and evaluate a Multi-Layer Perceptron (Neural Network).\n",
    "6. Compare the performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(feature_train_full, label_train_full), (feature_test, label_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"Training data shape: {feature_train_full.shape}\")\n",
    "print(f\"Test data shape: {feature_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 5-10 random sample images\n",
    "num_samples = 10\n",
    "random_indices = np.random.choice(feature_train_full.shape[0], num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow(feature_train_full[idx], cmap='gray')\n",
    "    plt.title(f\"Label: {label_train_full[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing\n",
    "- Normalize pixel values to 0-1 range.\n",
    "- Flatten images from 28x28 to 784 features.\n",
    "- Split training data into training and validation sets (80/20) using stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "feature_train_full = feature_train_full.astype('float32') / 255.0\n",
    "feature_test = feature_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "feature_train_flat = feature_train_full.reshape(feature_train_full.shape[0], -1)\n",
    "feature_test_flat = feature_test.reshape(feature_test.shape[0], -1)\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "feature_train, feature_val, label_train, label_val = train_test_split(\n",
    "    feature_train_flat, label_train_full,\n",
    "    test_size=0.2,\n",
    "    stratify=label_train_full,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Feature Train Shape: {feature_train.shape}\")\n",
    "print(f\"Feature Val Shape: {feature_val.shape}\")\n",
    "print(f\"Feature Test Shape: {feature_test_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Class Distributions\n",
    "def plot_distribution(labels, title):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    plt.bar(unique, counts)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(unique)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plot_distribution(label_train_full, 'Original Training Set')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plot_distribution(label_train, 'New Training Set')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plot_distribution(label_val, 'Validation Set')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plot_distribution(label_test, 'Test Set')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Decision Tree Classifier\n",
    "We will train a decision tree using Information Gain (criterion='entropy')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "dt_clf.fit(feature_train, label_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(f\"Validation Accuracy (Default DT): {dt_clf.score(feature_val, label_val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree (limited depth for readability)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_clf, max_depth=3, feature_names=[f\"pixel_{i}\" for i in range(784)], class_names=[str(i) for i in range(10)], filled=True, fontsize=10)\n",
    "plt.title(\"Decision Tree Visualization (Top 3 Levels)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning for Decision Tree\n",
    "We will tune `max_depth`, `min_samples_split`, and `min_samples_leaf` using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(criterion='entropy', random_state=42), param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Train with Grid Search\n",
    "grid_search.fit(feature_train, label_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_dt_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_dt_params)\n",
    "\n",
    "# Best Model\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "print(f\"Best Validation Accuracy: {best_dt_model.score(feature_val, label_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Neural Network Classifier\n",
    "We will use `MLPClassifier` with 2 hidden layers (128, 64 neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLP Classifier\n",
    "mlp_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=50,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train MLP\n",
    "mlp_clf.fit(feature_train, label_train)\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(mlp_clf.loss_curve_)\n",
    "plt.title(\"MLP Loss Curve\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Evaluation & Comparison\n",
    "Evaluating both the best Decision Tree and the Neural Network on the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Decision Tree\": best_dt_model, \"Neural Network\": mlp_clf}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "    y_pred = model.predict(feature_test_flat)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(label_test, y_pred)\n",
    "    prec = precision_score(label_test, y_pred, average='weighted')\n",
    "    rec = recall_score(label_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(label_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(label_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[str(i) for i in range(10)])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"Classification Report for {name}:\\n\")\n",
    "    print(classification_report(label_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Insights\n",
    "\n",
    "\n",
    "- **Which model performed better?**\n",
    "    - Typically, the Neural Network (MLP) outperforms the Decision Tree on image data like MNIST. Decision Trees struggle with pixel-level dependencies and translation invariance, whereas MLPs (even simple ones) can capture non-linear relationships better.\n",
    "\n",
    "- **Did the Decision Tree overfit?**\n",
    "    - Decision Trees tend to overfit if not pruned (max_depth restricted). The difference between training accuracy and validation accuracy would indicate overfitting. Tuning the hyperparameters helps mitigate this.\n",
    "\n",
    "- **Which digits were most commonly confused?**\n",
    "    - Common confusions in MNIST include 4 vs 9, 3 vs 5, or 8 vs 3. The Confusion Matrix above highlights the specific misclassifications for this run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
